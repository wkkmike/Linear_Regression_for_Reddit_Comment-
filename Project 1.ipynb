{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json # we need to use the JSON package to load the data, since the data is stored in JSON format\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import timeit\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lowercase the text and split by whitespace\n",
    "def preprocess_simple(data):\n",
    "    data1 = data.copy()\n",
    "    for data_point in data1:\n",
    "        data_point[\"text\"] = data_point[\"text\"].lower().split()\n",
    "    return data1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing using nltk package, tokenize, lemmatize, and remove unrelevant symbol\n",
    "def preprocess_new_feature(data):\n",
    "    data1 = data.copy()\n",
    "    wordnet_lemmatizer = WordNetLemmatizer()\n",
    "    for data_point in data:\n",
    "        data_point[\"text\"] = word_tokenize(data_point[\"text\"])\n",
    "        data_point[\"text\"] = [w.lower() for w in data_point[\"text\"]]\n",
    "        data_point[\"text\"] = [wordnet_lemmatizer.lemmatize(w) for w in data_point[\"text\"]]\n",
    "        data_point[\"text\"] = [w for w in data_point[\"text\"] if not w in ['.', ',', \"'\", '\"', \"?\", \"!\", \"[\", \"]\", '(', ')', '-', '...'\n",
    "           , \"''\", '``',\":\", \" \"]]\n",
    "    return data1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data to training, validation and testing sets.\n",
    "def split_data(data):\n",
    "    train_data = data[:10000]\n",
    "    val_data = data[10000:11000]\n",
    "    test_data = data[11000:]\n",
    "    return train_data, val_data, test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Return words count matrix\n",
    "# num specify top-n words count return\n",
    "def words_count(data, num=60):\n",
    "    words_recurrence = {}\n",
    "    for data_point in data:\n",
    "        for word in data_point[\"text\"]:\n",
    "            if word in words_recurrence:\n",
    "                words_recurrence[word] += 1\n",
    "            else:\n",
    "                words_recurrence[word] = 1\n",
    "    words_recurrence = sorted(words_recurrence.items(), key=lambda kv: kv[1], reverse=True)\n",
    "    words_recurrence = words_recurrence[:num]\n",
    "    return words_recurrence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get feature matrix given data.\n",
    "# Num specify the number of points.\n",
    "def feature_extraction(data, num, words_recurrence):\n",
    "    w = {}\n",
    "    i = 0\n",
    "    for word in words_recurrence:\n",
    "        w[word[0]] = i\n",
    "        i += 1\n",
    "    feature_num = len(words_recurrence)\n",
    "    x = np.zeros((num, feature_num))\n",
    "    i = 0\n",
    "    for data_point in data:\n",
    "        for word in data_point[\"text\"]:\n",
    "            if word in w:\n",
    "                x[i, w[word]] += 1\n",
    "        i += 1\n",
    "    return x;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add bias term to the data matrix\n",
    "def add_bias(matrix):\n",
    "    x_dataset_bias = np.ones((matrix.shape[0], matrix.shape[1]+1))\n",
    "    x_dataset_bias[:,:-1] = matrix\n",
    "    return x_dataset_bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the mean square error\n",
    "def mse(predict, true):\n",
    "    return (np.square(predict - true).mean(axis=None))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This cluster of code, will divided the data into training, validation and testing sets. and **print the top 160 words in the training sets.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['the',\n",
       " 'i',\n",
       " 'a',\n",
       " 'to',\n",
       " 'and',\n",
       " 'of',\n",
       " 'you',\n",
       " 'that',\n",
       " 'in',\n",
       " 'it',\n",
       " 'was',\n",
       " 'is',\n",
       " 'my',\n",
       " 'for',\n",
       " 'have',\n",
       " 'but',\n",
       " 'this',\n",
       " 'with',\n",
       " 'on',\n",
       " 'not',\n",
       " 'be',\n",
       " 'just',\n",
       " 'or',\n",
       " 'if',\n",
       " 'they',\n",
       " 'your',\n",
       " 'so',\n",
       " 'like',\n",
       " 'are',\n",
       " 'at',\n",
       " 'me',\n",
       " 'as',\n",
       " 'he',\n",
       " \"it's\",\n",
       " 'she',\n",
       " \"i'm\",\n",
       " 'about',\n",
       " 'when',\n",
       " 'we',\n",
       " 'all',\n",
       " 'because',\n",
       " 'from',\n",
       " 'her',\n",
       " 'out',\n",
       " 'would',\n",
       " 'get',\n",
       " \"don't\",\n",
       " 'had',\n",
       " 'what',\n",
       " 'one',\n",
       " 'up',\n",
       " 'people',\n",
       " 'can',\n",
       " 'an',\n",
       " 'do',\n",
       " '[deleted]',\n",
       " 'some',\n",
       " 'how',\n",
       " 'any',\n",
       " 'really',\n",
       " 'his',\n",
       " 'then',\n",
       " 'think',\n",
       " 'no',\n",
       " 'more',\n",
       " 'there',\n",
       " 'their',\n",
       " 'who',\n",
       " 'were',\n",
       " 'will',\n",
       " 'it.',\n",
       " 'them',\n",
       " 'by',\n",
       " 'has',\n",
       " 'only',\n",
       " 'time',\n",
       " 'go',\n",
       " 'know',\n",
       " 'even',\n",
       " 'got',\n",
       " 'been',\n",
       " 'good',\n",
       " 'new',\n",
       " 'after',\n",
       " \"you're\",\n",
       " 'never',\n",
       " 'please',\n",
       " 'make',\n",
       " 'than',\n",
       " 'am',\n",
       " 'still',\n",
       " \"that's\",\n",
       " 'him',\n",
       " 'did',\n",
       " 'being',\n",
       " 'into',\n",
       " 'much',\n",
       " \"didn't\",\n",
       " 'other',\n",
       " 'something',\n",
       " 'going',\n",
       " 'could',\n",
       " 'post',\n",
       " 'first',\n",
       " 'way',\n",
       " 'where',\n",
       " 'also',\n",
       " 'want',\n",
       " 'over',\n",
       " '-',\n",
       " 'most',\n",
       " \"can't\",\n",
       " 'say',\n",
       " 'see',\n",
       " 'pretty',\n",
       " 'why',\n",
       " 'season',\n",
       " 'back',\n",
       " \"i've\",\n",
       " 'very',\n",
       " 'always',\n",
       " 'made',\n",
       " 'love',\n",
       " 'our',\n",
       " 'questions',\n",
       " 'now',\n",
       " 'every',\n",
       " 'which',\n",
       " 'before',\n",
       " 'show',\n",
       " 'fucking',\n",
       " 'moderators',\n",
       " 'too',\n",
       " 'shit',\n",
       " 'actually',\n",
       " 'friends',\n",
       " 'off',\n",
       " 'fuck',\n",
       " 'years',\n",
       " 'feel',\n",
       " 'its',\n",
       " 'me.',\n",
       " 'should',\n",
       " 'same',\n",
       " 'use',\n",
       " 'things',\n",
       " 'few',\n",
       " 'lot',\n",
       " 'action',\n",
       " '[contact',\n",
       " 'life',\n",
       " \"doesn't\",\n",
       " 'best',\n",
       " 'those',\n",
       " 'thing',\n",
       " 'said',\n",
       " 'while',\n",
       " 'someone',\n",
       " 'performed',\n",
       " 'right']"
      ]
     },
     "execution_count": 206,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Open the file\n",
    "with open(\"proj1_data.json\") as fp:\n",
    "    data = json.load(fp)\n",
    "data_processed = preprocess_simple(data)\n",
    "\n",
    "# split data\n",
    "train_data, val_data, test_data = split_data(data_processed)\n",
    "words_top160 = words_count(train_data, num=160)\n",
    "words_list = []\n",
    "for word in words_top160:\n",
    "    words_list.append(word[0])\n",
    "words_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Closed form solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Closed form solution for linear regression\n",
    "def closed_form(X, y):\n",
    "    #return np.linalg.inv(np.transpose(X) @ X) @ np.transpose(X) @ y\n",
    "    return np.dot(np.dot(np.linalg.inv(np.dot(np.transpose(X), X)), np.transpose(X)), y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient descent solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gradient descent solution for linear regression\n",
    "def gradient_descent(X, y, beta, n0, eps, show=False):\n",
    "    i = 1\n",
    "    weight = np.random.rand(X.shape[1])\n",
    "    a1 = np.transpose(X) @ X\n",
    "    a2 = np.transpose(X) @ y\n",
    "    while True:\n",
    "        alpha = n0/((1+beta*i) * 10000)\n",
    "        weight_new = weight - 2*alpha * (a1@weight - a2)\n",
    "        difference = np.linalg.norm(weight_new - weight, ord=2)\n",
    "        weight = weight_new\n",
    "        i += 1   \n",
    "        if show:\n",
    "            if(i % 1000 == 0):\n",
    "                print(\"Iteration: \" + str(i) + \". Difference: \" + str(difference))\n",
    "        if (difference < eps):\n",
    "            break\n",
    "    return weight"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "construct matrix for 160 features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"proj1_data.json\") as fp:\n",
    "    data = json.load(fp)\n",
    "data_processed = preprocess_simple(data)\n",
    "\n",
    "# split data\n",
    "train_data, val_data, test_data = split_data(data_processed)\n",
    "words_top160 = words_count(train_data, num=160)\n",
    "train_x_160 = add_bias(feature_extraction(train_data, 10000, words_top160))\n",
    "val_x_160 = add_bias(feature_extraction(val_data, 1000, words_top160))\n",
    "test_x_160 = add_bias(feature_extraction(test_data, 1000, words_top160))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "construct matrix for 60 features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"proj1_data.json\") as fp:\n",
    "    data = json.load(fp)\n",
    "data_processed = preprocess_simple(data)\n",
    "\n",
    "# split data\n",
    "train_data, val_data, test_data = split_data(data_processed)\n",
    "words_top60 = words_count(train_data, num=60)\n",
    "train_x_60 = add_bias(feature_extraction(train_data, 10000, words_top60))\n",
    "val_x_60 = add_bias(feature_extraction(val_data, 1000, words_top60))\n",
    "test_x_60 = add_bias(feature_extraction(test_data, 1000, words_top60))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "construct matrix for 3 features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"proj1_data.json\") as fp:\n",
    "    data = json.load(fp)\n",
    "x_3 = np.zeros((12000, 3))\n",
    "i=0\n",
    "for data_point in data:\n",
    "    if data_point[\"is_root\"]:\n",
    "        x_3[i, 0] = 1\n",
    "    else:\n",
    "        x_3[i, 0] = 0\n",
    "    x_3[i, 1] = data_point[\"controversiality\"]\n",
    "    x_3[i, 2] = data_point[\"children\"]\n",
    "    i = i + 1\n",
    "train_x_3 = x_3[:10000]\n",
    "val_x_3 = x_3[10000:11000]\n",
    "test_x_3 = x_3[11000:]\n",
    "train_x_3 = add_bias(train_x_3)\n",
    "val_x_3 = add_bias(val_x_3)\n",
    "test_x_3 = add_bias(test_x_3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Construct y-matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"proj1_data.json\") as fp:\n",
    "    data = json.load(fp)\n",
    "y = np.zeros(12000)\n",
    "i = 0\n",
    "for data_point in data:\n",
    "    y[i] = data_point[\"popularity_score\"]\n",
    "    i += 1\n",
    "y_train = y[:10000]\n",
    "y_val = y[10000:11000]\n",
    "y_test = y[11000:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running time comparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 1000. Difference: 0.00013347617708752714\n",
      "Iteration: 2000. Difference: 5.4701409404181445e-05\n",
      "Iteration: 3000. Difference: 3.208967427699064e-05\n",
      "Iteration: 4000. Difference: 2.192301947099826e-05\n",
      "Iteration: 5000. Difference: 1.6303433688352737e-05\n",
      "Iteration: 6000. Difference: 1.279869905921143e-05\n",
      "Iteration: 7000. Difference: 1.0431821603347455e-05\n",
      "Iteration: 8000. Difference: 8.740373404975856e-06\n",
      "Iteration: 9000. Difference: 7.479382742353368e-06\n",
      "Iteration: 10000. Difference: 6.507909934180241e-06\n",
      "Iteration: 11000. Difference: 5.739573358064625e-06\n",
      "Iteration: 12000. Difference: 5.118709723029328e-06\n",
      "Iteration: 13000. Difference: 4.60795996019291e-06\n",
      "Iteration: 14000. Difference: 4.181389322726661e-06\n",
      "Iteration: 15000. Difference: 3.820474228525257e-06\n",
      "Iteration: 16000. Difference: 3.5116587489986417e-06\n",
      "Iteration: 17000. Difference: 3.2448115573254455e-06\n",
      "Iteration: 18000. Difference: 3.01222041477776e-06\n",
      "Iteration: 19000. Difference: 2.807918772025031e-06\n",
      "Iteration: 20000. Difference: 2.627223818921458e-06\n",
      "Iteration: 21000. Difference: 2.46641276855797e-06\n",
      "Iteration: 22000. Difference: 2.322491657864226e-06\n",
      "Iteration: 23000. Difference: 2.193027383893273e-06\n",
      "Iteration: 24000. Difference: 2.0760237846703995e-06\n",
      "Iteration: 25000. Difference: 1.969828928562169e-06\n",
      "Iteration: 26000. Difference: 1.8730648618700924e-06\n",
      "Iteration: 27000. Difference: 1.7845737498452156e-06\n",
      "Iteration: 28000. Difference: 1.7033761403374822e-06\n",
      "Iteration: 29000. Difference: 1.6286382999121383e-06\n",
      "Iteration: 30000. Difference: 1.5596464153548208e-06\n",
      "Iteration: 31000. Difference: 1.495786043887471e-06\n",
      "Iteration: 32000. Difference: 1.4365256143127206e-06\n",
      "Iteration: 33000. Difference: 1.3814030826775997e-06\n",
      "Iteration: 34000. Difference: 1.3300150642782505e-06\n",
      "Iteration: 35000. Difference: 1.282007925959013e-06\n",
      "Iteration: 36000. Difference: 1.2370704397370677e-06\n",
      "Iteration: 37000. Difference: 1.1949276903758394e-06\n",
      "Iteration: 38000. Difference: 1.1553359946101681e-06\n",
      "Iteration: 39000. Difference: 1.1180786435689781e-06\n",
      "Iteration: 40000. Difference: 1.0829623169958113e-06\n",
      "Iteration: 41000. Difference: 1.0498140498800426e-06\n",
      "Iteration: 42000. Difference: 1.0184786553033928e-06\n"
     ]
    }
   ],
   "source": [
    "start = timeit.default_timer()\n",
    "weights_cf = closed_form(train_x_3, y_train)\n",
    "end = timeit.default_timer()\n",
    "run_time_cf = end - start\n",
    "start = timeit.default_timer()\n",
    "weights_gd = gradient_descent(train_x_3, y_train, beta=10e-3, n0=10e-3, eps= 1 * 10e-07, show=True)\n",
    "end = timeit.default_timer()\n",
    "run_time_gd = end - start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running time for closed form with 3 features is: 0.003852369998639915s.\n",
      "Running time for gradient descent with 3 features is : 0.8398717099989881s.\n"
     ]
    }
   ],
   "source": [
    "print(\"Running time for closed form with 3 features is: \" + str(run_time_cf) + \"s.\")\n",
    "print(\"Running time for gradient descent with 3 features is : \" + str(run_time_gd) + \"s.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MSE comparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [],
   "source": [
    "mse_cf = mse(np.dot(val_x_3, weights_cf), y_val)\n",
    "mse_gd = mse(np.dot(val_x_3, weights_gd), y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE for closed form with 3 features is: 1.0203266848431447\n",
      "MSE for gradient descent with 3 features is: 1.0374712327967792\n"
     ]
    }
   ],
   "source": [
    "print(\"MSE for closed form with 3 features is: \" + str(mse_cf))\n",
    "print(\"MSE for gradient descent with 3 features is: \" + str(mse_gd))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run 5 times for closed form:\n",
      "[-0.22627679 -1.08584747  0.37536403  0.82092517]\n",
      "[-0.22627679 -1.08584747  0.37536403  0.82092517]\n",
      "[-0.22627679 -1.08584747  0.37536403  0.82092517]\n",
      "[-0.22627679 -1.08584747  0.37536403  0.82092517]\n",
      "[-0.22627679 -1.08584747  0.37536403  0.82092517]\n",
      "Run 5 times for gradient descent (same hyperparameter):\n",
      "[-0.14224083  0.51787342  0.37488564  0.75887071]\n",
      "[-0.17700867  0.24258613  0.37393427  0.78069055]\n",
      "[-0.15683294  0.408893    0.37446678  0.7679638 ]\n",
      "[-0.17129554 -0.05893925  0.37511668  0.78054847]\n",
      "[-0.12348998  0.27678157  0.37655113  0.7509721 ]\n"
     ]
    }
   ],
   "source": [
    "print(\"Run 5 times for closed form:\")\n",
    "for i in range(5):\n",
    "    weights_cf = closed_form(train_x_3, y_train)\n",
    "    print(weights_cf)\n",
    "print(\"Run 5 times for gradient descent (same hyperparameter):\")\n",
    "for i in range(5):\n",
    "    weights_gd = gradient_descent(train_x_3, y_train, beta=10e-3, n0=10e-3, eps= 1 * 10e-07)\n",
    "    print(weights_gd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Performance comparasion for different feature"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use **closed form** in our comparasion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights_3 = closed_form(train_x_3, y_train)\n",
    "weights_60 = closed_form(train_x_60, y_train)\n",
    "weights_160 = closed_form(train_x_160, y_train)\n",
    "mse_3_train = mse(np.dot(train_x_3, weights_3), y_train)\n",
    "mse_3_val = mse(np.dot(val_x_3, weights_3), y_val)\n",
    "mse_3_test = mse(np.dot(test_x_3, weights_3), y_test)\n",
    "mse_60_train = mse(np.dot(train_x_60, weights_60), y_train)\n",
    "mse_60_val = mse(np.dot(val_x_60, weights_60), y_val)\n",
    "mse_60_test = mse(np.dot(test_x_60, weights_60), y_test)\n",
    "mse_160_train = mse(np.dot(train_x_160, weights_160), y_train)\n",
    "mse_160_val = mse(np.dot(val_x_160, weights_160), y_val)\n",
    "mse_160_test = mse(np.dot(test_x_160, weights_160), y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare mse for train sets, validation sets for different feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE for training sets with no text feature:1.0846830709157251\n",
      "MSE for validation sets with no text feature:1.0203266848431447\n",
      "MSE for test sets with no text feature:1.2975311523528352\n",
      "MSE for training sets with top 60 words:1.3357113742096964\n",
      "MSE for validation sets with top 60 words:1.2652963638167058\n",
      "MSE for test sets with top 60 words:1.5782224030741834\n",
      "MSE for training sets with top 160 words:1.3179296503478746\n",
      "MSE for validation sets with top 160 words:1.2917631409955967\n",
      "MSE for test sets with top 160 words:1.6250688520751755\n"
     ]
    }
   ],
   "source": [
    "print(\"MSE for training sets with no text feature:\" + str(mse_3_train))\n",
    "print(\"MSE for validation sets with no text feature:\" + str(mse_3_val))\n",
    "print(\"MSE for test sets with no text feature:\" + str(mse_3_test))\n",
    "print(\"MSE for training sets with top 60 words:\" + str(mse_60_train))\n",
    "print(\"MSE for validation sets with top 60 words:\" + str(mse_60_val))\n",
    "print(\"MSE for test sets with top 60 words:\" + str(mse_60_test))\n",
    "print(\"MSE for training sets with top 160 words:\" + str(mse_160_train))\n",
    "print(\"MSE for validation sets with top 160 words:\" + str(mse_160_val))\n",
    "print(\"MSE for test sets with top 160 words:\" + str(mse_160_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Second new feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add the second new feature\n",
    "def add_newfeature(x_matrix,data):\n",
    "    x_new = np.zeros(12000)\n",
    "    j = 0\n",
    "    for data_point in data:\n",
    "        x_new[j] = (data_point[\"children\"]**2+data_point[\"controversiality\"]**3)\n",
    "        j += 1\n",
    "    data1=np.append(x_matrix,x_new.reshape(12000,1),axis = 1)\n",
    "    return(data1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct matrx x_feature2\n",
    "x_f2_60 = feature_extraction(data_processed, 12000, words_top60)\n",
    "x_f2 = add_bias(add_newfeature(x_f2_60, data))\n",
    "# split x_feature2\n",
    "x_f2_train = x_f2[:10000]\n",
    "x_f2_val = x_f2[10000:11000]\n",
    "x_f2_test = x_f2[11000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [],
   "source": [
    "weight_f2_60 = closed_form(x_f2_train, y_train)\n",
    "mse_60_f2_train = mse(np.dot(x_f2_train, weight_f2_60), y_train)\n",
    "mse_60_f2_val = mse(np.dot(x_f2_val, weight_f2_60), y_val)\n",
    "mse_60_f2_test = mse(np.dot(x_f2_test, weight_f2_60), y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE for training sets with feature2:1.254839390648869\n",
      "MSE for validation sets with feature2:1.1692346565730596\n",
      "MSE for test sets with feature2:1.5184938588610217\n"
     ]
    }
   ],
   "source": [
    "print(\"MSE for training sets with feature2:\" + str(mse_60_f2_train))\n",
    "print(\"MSE for validation sets with feature2:\" + str(mse_60_f2_val))\n",
    "print(\"MSE for test sets with feature2:\" + str(mse_60_f2_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
