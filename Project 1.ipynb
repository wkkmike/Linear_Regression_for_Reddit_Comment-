{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json # we need to use the JSON package to load the data, since the data is stored in JSON format\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import timeit\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lowercase the text and split by whitespace\n",
    "def preprocess_simple(data):\n",
    "    data1 = data.copy()\n",
    "    for data_point in data1:\n",
    "        data_point[\"text\"] = data_point[\"text\"].lower().split()\n",
    "    return data1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing using nltk package, tokenize, lemmatize, and remove unrelevant symbol\n",
    "def preprocess_new_feature(data):\n",
    "    data1 = data.copy()\n",
    "    wordnet_lemmatizer = WordNetLemmatizer()\n",
    "    for data_point in data:\n",
    "        data_point[\"text\"] = word_tokenize(data_point[\"text\"])\n",
    "        data_point[\"text\"] = [w.lower() for w in data_point[\"text\"]]\n",
    "        data_point[\"text\"] = [wordnet_lemmatizer.lemmatize(w) for w in data_point[\"text\"]]\n",
    "        data_point[\"text\"] = [w for w in data_point[\"text\"] if not w in ['.', ',', \"'\", '\"', \"?\", \"!\", \"[\", \"]\", '(', ')', '-', '...'\n",
    "           , \"''\", '``',\":\", \" \"]]\n",
    "    return data1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data to training, validation and testing sets.\n",
    "def split_data(data):\n",
    "    train_data = data[:10000]\n",
    "    val_data = data[10000:11000]\n",
    "    test_data = data[11000:]\n",
    "    return train_data, val_data, test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Return words count matrix\n",
    "# num specify top-n words count return\n",
    "def words_count(data, num=60):\n",
    "    words_recurrence = {}\n",
    "    for data_point in data:\n",
    "        for word in data_point[\"text\"]:\n",
    "            if word in words_recurrence:\n",
    "                words_recurrence[word] += 1\n",
    "            else:\n",
    "                words_recurrence[word] = 1\n",
    "    words_recurrence = sorted(words_recurrence.items(), key=lambda kv: kv[1], reverse=True)\n",
    "    words_recurrence = words_recurrence[:num]\n",
    "    return words_recurrence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get feature matrix given data.\n",
    "# Num specify the number of points.\n",
    "def feature_extraction(data, num, words_recurrence):\n",
    "    w = {}\n",
    "    i = 0\n",
    "    for word in words_recurrence:\n",
    "        w[word[0]] = i\n",
    "        i += 1\n",
    "    feature_num = len(words_recurrence)\n",
    "    x = np.zeros((num, feature_num))\n",
    "    i = 0\n",
    "    for data_point in data:\n",
    "        for word in data_point[\"text\"]:\n",
    "            if word in w:\n",
    "                x[i, w[word]] += 1\n",
    "        i += 1\n",
    "    return x;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add bias term to the data matrix\n",
    "def add_bias(matrix):\n",
    "    x_dataset_bias = np.ones((matrix.shape[0], matrix.shape[1]+1))\n",
    "    x_dataset_bias[:,:-1] = matrix\n",
    "    return x_dataset_bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the mean square error\n",
    "def mse(predict, true):\n",
    "    return (np.square(predict - true).mean(axis=None))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This cluster of code, will divided the data into training, validation and testing sets. and **print the top 160 words in the training sets.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['the',\n",
       " 'i',\n",
       " 'a',\n",
       " 'to',\n",
       " 'and',\n",
       " 'of',\n",
       " 'you',\n",
       " 'that',\n",
       " 'in',\n",
       " 'it',\n",
       " 'was',\n",
       " 'is',\n",
       " 'my',\n",
       " 'for',\n",
       " 'have',\n",
       " 'but',\n",
       " 'this',\n",
       " 'with',\n",
       " 'on',\n",
       " 'not',\n",
       " 'be',\n",
       " 'just',\n",
       " 'or',\n",
       " 'if',\n",
       " 'they',\n",
       " 'your',\n",
       " 'so',\n",
       " 'like',\n",
       " 'are',\n",
       " 'at',\n",
       " 'me',\n",
       " 'as',\n",
       " 'he',\n",
       " \"it's\",\n",
       " 'she',\n",
       " \"i'm\",\n",
       " 'about',\n",
       " 'when',\n",
       " 'we',\n",
       " 'all',\n",
       " 'because',\n",
       " 'from',\n",
       " 'her',\n",
       " 'out',\n",
       " 'would',\n",
       " 'get',\n",
       " \"don't\",\n",
       " 'had',\n",
       " 'what',\n",
       " 'one',\n",
       " 'up',\n",
       " 'people',\n",
       " 'can',\n",
       " 'an',\n",
       " 'do',\n",
       " '[deleted]',\n",
       " 'some',\n",
       " 'how',\n",
       " 'any',\n",
       " 'really',\n",
       " 'his',\n",
       " 'then',\n",
       " 'think',\n",
       " 'no',\n",
       " 'more',\n",
       " 'there',\n",
       " 'their',\n",
       " 'who',\n",
       " 'were',\n",
       " 'will',\n",
       " 'it.',\n",
       " 'them',\n",
       " 'by',\n",
       " 'has',\n",
       " 'only',\n",
       " 'time',\n",
       " 'go',\n",
       " 'know',\n",
       " 'even',\n",
       " 'got',\n",
       " 'been',\n",
       " 'good',\n",
       " 'new',\n",
       " 'after',\n",
       " \"you're\",\n",
       " 'never',\n",
       " 'please',\n",
       " 'make',\n",
       " 'than',\n",
       " 'am',\n",
       " 'still',\n",
       " \"that's\",\n",
       " 'him',\n",
       " 'did',\n",
       " 'being',\n",
       " 'into',\n",
       " 'much',\n",
       " \"didn't\",\n",
       " 'other',\n",
       " 'something',\n",
       " 'going',\n",
       " 'could',\n",
       " 'post',\n",
       " 'first',\n",
       " 'way',\n",
       " 'where',\n",
       " 'also',\n",
       " 'want',\n",
       " 'over',\n",
       " '-',\n",
       " 'most',\n",
       " \"can't\",\n",
       " 'say',\n",
       " 'see',\n",
       " 'pretty',\n",
       " 'why',\n",
       " 'season',\n",
       " 'back',\n",
       " \"i've\",\n",
       " 'very',\n",
       " 'always',\n",
       " 'made',\n",
       " 'love',\n",
       " 'our',\n",
       " 'questions',\n",
       " 'now',\n",
       " 'every',\n",
       " 'which',\n",
       " 'before',\n",
       " 'show',\n",
       " 'fucking',\n",
       " 'moderators',\n",
       " 'too',\n",
       " 'shit',\n",
       " 'actually',\n",
       " 'friends',\n",
       " 'off',\n",
       " 'fuck',\n",
       " 'years',\n",
       " 'feel',\n",
       " 'its',\n",
       " 'me.',\n",
       " 'should',\n",
       " 'same',\n",
       " 'use',\n",
       " 'things',\n",
       " 'few',\n",
       " 'lot',\n",
       " 'action',\n",
       " '[contact',\n",
       " 'life',\n",
       " \"doesn't\",\n",
       " 'best',\n",
       " 'those',\n",
       " 'thing',\n",
       " 'said',\n",
       " 'while',\n",
       " 'someone',\n",
       " 'performed',\n",
       " 'right']"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Open the file\n",
    "with open(\"proj1_data.json\") as fp:\n",
    "    data = json.load(fp)\n",
    "data_processed = preprocess_simple(data)\n",
    "\n",
    "# split data\n",
    "train_data, val_data, test_data = split_data(data_processed)\n",
    "words_top160 = words_count(train_data, num=160)\n",
    "words_list = []\n",
    "for word in words_top160:\n",
    "    words_list.append(word[0])\n",
    "words_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Closed form solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Closed form solution for linear regression\n",
    "def closed_form(X, y):\n",
    "    #return np.linalg.inv(np.transpose(X) @ X) @ np.transpose(X) @ y\n",
    "    return np.dot(np.dot(np.linalg.inv(np.dot(np.transpose(X), X)), np.transpose(X)), y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient descent solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gradient descent solution for linear regression\n",
    "def gradient_descent(X, y, beta, n0, eps, show=False):\n",
    "    i = 1\n",
    "    weight = np.random.rand(X.shape[1])\n",
    "    a1 = np.transpose(X) @ X\n",
    "    a2 = np.transpose(X) @ y\n",
    "    while True:\n",
    "        alpha = n0/((1+beta*i) * 10000)\n",
    "        weight_new = weight - 2*alpha * (a1@weight - a2)\n",
    "        difference = np.linalg.norm(weight_new - weight, ord=2)\n",
    "        weight = weight_new\n",
    "        i += 1   \n",
    "        if show:\n",
    "            if(i % 1000 == 0):\n",
    "                print(\"Iteration: \" + str(i) + \". Difference: \" + str(difference))\n",
    "        if (difference < eps):\n",
    "            break\n",
    "    return weight"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "construct matrix for 160 features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"proj1_data.json\") as fp:\n",
    "    data = json.load(fp)\n",
    "data_processed = preprocess_simple(data)\n",
    "\n",
    "# split data\n",
    "train_data, val_data, test_data = split_data(data_processed)\n",
    "words_top160 = words_count(train_data, num=160)\n",
    "train_x_160 = add_bias(feature_extraction(train_data, 10000, words_top160))\n",
    "val_x_160 = add_bias(feature_extraction(val_data, 1000, words_top160))\n",
    "test_x_160 = add_bias(feature_extraction(test_data, 1000, words_top160))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "construct matrix for 60 features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"proj1_data.json\") as fp:\n",
    "    data = json.load(fp)\n",
    "data_processed = preprocess_simple(data)\n",
    "\n",
    "# split data\n",
    "train_data, val_data, test_data = split_data(data_processed)\n",
    "words_top60 = words_count(train_data, num=60)\n",
    "train_x_60 = add_bias(feature_extraction(train_data, 10000, words_top60))\n",
    "val_x_60 = add_bias(feature_extraction(val_data, 1000, words_top60))\n",
    "test_x_60 = add_bias(feature_extraction(test_data, 1000, words_top60))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "construct matrix for 3 features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"proj1_data.json\") as fp:\n",
    "    data = json.load(fp)\n",
    "x_3 = np.zeros((12000, 3))\n",
    "i=0\n",
    "for data_point in data:\n",
    "    if data_point[\"is_root\"]:\n",
    "        x_3[i, 0] = 1\n",
    "    else:\n",
    "        x_3[i, 0] = 0\n",
    "    x_3[i, 1] = data_point[\"controversiality\"]\n",
    "    x_3[i, 2] = data_point[\"children\"]\n",
    "    i = i + 1\n",
    "train_x_3 = x_3[:10000]\n",
    "val_x_3 = x_3[10000:11000]\n",
    "test_x_3 = x_3[11000:]\n",
    "train_x_3 = add_bias(train_x_3)\n",
    "val_x_3 = add_bias(val_x_3)\n",
    "test_x_3 = add_bias(test_x_3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Construct y-matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"proj1_data.json\") as fp:\n",
    "    data = json.load(fp)\n",
    "y = np.zeros(12000)\n",
    "i = 0\n",
    "for data_point in data:\n",
    "    y[i] = data_point[\"popularity_score\"]\n",
    "    i += 1\n",
    "y_train = y[:10000]\n",
    "y_val = y[10000:11000]\n",
    "y_test = y[11000:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running time comparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 1000. Difference: 0.00015673044977630802\n",
      "Iteration: 2000. Difference: 6.472833334567594e-05\n",
      "Iteration: 3000. Difference: 3.8169431291888766e-05\n",
      "Iteration: 4000. Difference: 2.619291552084706e-05\n",
      "Iteration: 5000. Difference: 1.955659561260473e-05\n",
      "Iteration: 6000. Difference: 1.5408475534209258e-05\n",
      "Iteration: 7000. Difference: 1.260123544594461e-05\n",
      "Iteration: 8000. Difference: 1.0591127106886764e-05\n",
      "Iteration: 9000. Difference: 9.089756532183915e-06\n",
      "Iteration: 10000. Difference: 7.931015766893505e-06\n",
      "Iteration: 11000. Difference: 7.012988581357301e-06\n",
      "Iteration: 12000. Difference: 6.26993372314814e-06\n",
      "Iteration: 13000. Difference: 5.6576842529304175e-06\n",
      "Iteration: 14000. Difference: 5.145549775031771e-06\n",
      "Iteration: 15000. Difference: 4.7115894529005165e-06\n",
      "Iteration: 16000. Difference: 4.339731842562988e-06\n",
      "Iteration: 17000. Difference: 4.017954853829322e-06\n",
      "Iteration: 18000. Difference: 3.737098949840631e-06\n",
      "Iteration: 19000. Difference: 3.4900718205898256e-06\n",
      "Iteration: 20000. Difference: 3.2713024423606684e-06\n",
      "Iteration: 21000. Difference: 3.0763582645186973e-06\n",
      "Iteration: 22000. Difference: 2.9016716354169785e-06\n",
      "Iteration: 23000. Difference: 2.7443409330318784e-06\n",
      "Iteration: 24000. Difference: 2.6019837575918042e-06\n",
      "Iteration: 25000. Difference: 2.4726270327013937e-06\n",
      "Iteration: 26000. Difference: 2.354623680718214e-06\n",
      "Iteration: 27000. Difference: 2.2465887049349036e-06\n",
      "Iteration: 28000. Difference: 2.147349631506333e-06\n",
      "Iteration: 29000. Difference: 2.0559077022552363e-06\n",
      "Iteration: 30000. Difference: 1.9714072075036474e-06\n",
      "Iteration: 31000. Difference: 1.8931110448484822e-06\n",
      "Iteration: 32000. Difference: 1.8203810857440196e-06\n",
      "Iteration: 33000. Difference: 1.7526622873274804e-06\n",
      "Iteration: 34000. Difference: 1.689469746245893e-06\n",
      "Iteration: 35000. Difference: 1.6303780821145146e-06\n",
      "Iteration: 36000. Difference: 1.575012676953235e-06\n",
      "Iteration: 37000. Difference: 1.5230424067303708e-06\n",
      "Iteration: 38000. Difference: 1.4741735769302409e-06\n",
      "Iteration: 39000. Difference: 1.4281448378481701e-06\n",
      "Iteration: 40000. Difference: 1.3847229000848008e-06\n",
      "Iteration: 41000. Difference: 1.3436989088587798e-06\n",
      "Iteration: 42000. Difference: 1.3048853615777761e-06\n",
      "Iteration: 43000. Difference: 1.2681134770334966e-06\n",
      "Iteration: 44000. Difference: 1.233230941692651e-06\n",
      "Iteration: 45000. Difference: 1.2000999707278542e-06\n",
      "Iteration: 46000. Difference: 1.1685956348634668e-06\n",
      "Iteration: 47000. Difference: 1.1386044122289453e-06\n",
      "Iteration: 48000. Difference: 1.1100229286255577e-06\n",
      "Iteration: 49000. Difference: 1.0827568614298293e-06\n",
      "Iteration: 50000. Difference: 1.0567199803741328e-06\n",
      "Iteration: 51000. Difference: 1.0318333074120297e-06\n",
      "Iteration: 52000. Difference: 1.0080243783943856e-06\n"
     ]
    }
   ],
   "source": [
    "start = timeit.default_timer()\n",
    "weights_cf = closed_form(train_x_3, y_train)\n",
    "end = timeit.default_timer()\n",
    "run_time_cf = end - start\n",
    "start = timeit.default_timer()\n",
    "weights_gd = gradient_descent(train_x_3, y_train, beta=10e-3, n0=10e-3, eps= 1 * 10e-07, show=True)\n",
    "end = timeit.default_timer()\n",
    "run_time_gd = end - start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running time for closed form with 3 features is: 0.00058034659605255s.\n",
      "Running time for gradient descent with 3 features is : 0.4716558564441584s.\n"
     ]
    }
   ],
   "source": [
    "print(\"Running time for closed form with 3 features is: \" + str(run_time_cf) + \"s.\")\n",
    "print(\"Running time for gradient descent with 3 features is : \" + str(run_time_gd) + \"s.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MSE comparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "mse_cf = mse(np.dot(val_x_3, weights_cf), y_val)\n",
    "mse_gd = mse(np.dot(val_x_3, weights_gd), y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE for closed form with 3 features is: 1.0203266848431447\n",
      "MSE for gradient descent with 3 features is: 1.0511459440423714\n"
     ]
    }
   ],
   "source": [
    "print(\"MSE for closed form with 3 features is: \" + str(mse_cf))\n",
    "print(\"MSE for gradient descent with 3 features is: \" + str(mse_gd))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run 5 times for closed form:\n",
      "[-0.22627679 -1.08584747  0.37536403  0.82092517]\n",
      "[-0.22627679 -1.08584747  0.37536403  0.82092517]\n",
      "[-0.22627679 -1.08584747  0.37536403  0.82092517]\n",
      "[-0.22627679 -1.08584747  0.37536403  0.82092517]\n",
      "[-0.22627679 -1.08584747  0.37536403  0.82092517]\n",
      "Run 5 times for gradient descent (same hyperparameter):\n",
      "[-0.12245861  0.3295951   0.37644725  0.74988146]\n",
      "[-0.16823466  0.40122809  0.37391072  0.77429809]\n",
      "[-0.13625649  0.14536742  0.3762923   0.75928355]\n",
      "[-0.142403    0.37862931  0.3752892   0.76034302]\n",
      "[-0.15684602  0.18040528  0.37514236  0.77023957]\n"
     ]
    }
   ],
   "source": [
    "print(\"Run 5 times for closed form:\")\n",
    "for i in range(5):\n",
    "    weights_cf = closed_form(train_x_3, y_train)\n",
    "    print(weights_cf)\n",
    "print(\"Run 5 times for gradient descent (same hyperparameter):\")\n",
    "for i in range(5):\n",
    "    weights_gd = gradient_descent(train_x_3, y_train, beta=10e-3, n0=10e-3, eps= 1 * 10e-07)\n",
    "    print(weights_gd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Performance comparasion for different feature"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use **closed form** in our comparasion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights_3 = closed_form(train_x_3, y_train)\n",
    "weights_60 = closed_form(train_x_60, y_train)\n",
    "weights_160 = closed_form(train_x_160, y_train)\n",
    "mse_3_train = mse(np.dot(train_x_3, weights_3), y_train)\n",
    "mse_3_val = mse(np.dot(val_x_3, weights_3), y_val)\n",
    "mse_60_train = mse(np.dot(train_x_60, weights_60), y_train)\n",
    "mse_60_val = mse(np.dot(val_x_60, weights_60), y_val)\n",
    "mse_160_train = mse(np.dot(train_x_160, weights_160), y_train)\n",
    "mse_160_val = mse(np.dot(val_x_160, weights_160), y_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare mse for train sets, validation sets for different feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE for training sets with no text feature:1.0846830709157251\n",
      "MSE for validation sets with no text feature:1.0203266848431447\n",
      "MSE for training sets with top 60 words:1.3357113742096964\n",
      "MSE for validation sets with top 60 words:1.2652963638167056\n",
      "MSE for training sets with top 160 words:1.3179296503478746\n",
      "MSE for validation sets with top 160 words:1.2917631409955967\n"
     ]
    }
   ],
   "source": [
    "print(\"MSE for training sets with no text feature:\" + str(mse_3_train))\n",
    "print(\"MSE for validation sets with no text feature:\" + str(mse_3_val))\n",
    "print(\"MSE for training sets with top 60 words:\" + str(mse_60_train))\n",
    "print(\"MSE for validation sets with top 60 words:\" + str(mse_60_val))\n",
    "print(\"MSE for training sets with top 160 words:\" + str(mse_160_train))\n",
    "print(\"MSE for validation sets with top 160 words:\" + str(mse_160_val))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
